# ğŸ“„ SmartDoc QA

An AI-powered document question-answering system that uses **Retrieval-Augmented Generation (RAG)**, up to **2 LLMs**, **tool calling**, and supports **monitoring**, **evaluation**, and **deployment**.

---

## ğŸš€ Features

- âœ… Upload documents (PDF, Word)  
- ğŸ” Retrieve relevant context using vector search  
- ğŸ§  Generate accurate answers with LLMs  
- ğŸ› ï¸ Tool calling (e.g., calculator, summarizer, web search)  
- ğŸ§ª Offline & online evaluation support  
- ğŸ“Š Real-time monitoring of performance  
- ğŸŒ Deployable via Streamlit, Docker, or HuggingFace Spaces  

---

## ğŸ§  Architecture Overview

1. **Document Ingestion**  
   - Convert uploaded files to text  
   - Chunk and embed using transformer models  

2. **Retrieval**  
   - FAISS or Chroma vector DB for similarity search  

3. **Answer Generation**  
   - Primary LLM + optional secondary LLM  
   - Injects retrieved chunks into prompt context  

4. **Tool Calling**  
   - Invokes custom tools based on query need  

5. **Monitoring**  
   - Logs latency, token usage, and tool calls  

6. **Evaluation**  
   - Test sets, user feedback, and automatic metrics  

---

## ğŸ“‚ Project Structure

```
smartdoc-qa/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # Entry API or orchestration script
â”‚   â”œâ”€â”€ rag_pipeline.py      # RAG flow logic
â”‚   â”œâ”€â”€ tool_router.py       # Tool invocation handler
â”‚   â”œâ”€â”€ embeddings.py        # Text splitting & embedding
â”‚   â”œâ”€â”€ llm_wrapper.py       # LLM abstraction layer
â”‚   â”œâ”€â”€ evaluation.py        # Offline evaluation tools
â”‚   â””â”€â”€ monitor.py           # Logging, tracing
â”‚
â”œâ”€â”€ docs/                    # Sample documents
â”œâ”€â”€ logs/                    # Monitoring logs
â”œâ”€â”€ tests/                   # Evaluation test queries
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py     # Web UI
â”œâ”€â”€ .env                     # API keys and secrets
â”œâ”€â”€ Dockerfile               # For containerized deployment
â””â”€â”€ README.md
```

---

## ğŸ§° Tech Stack

| Category         | Tool/Tech                         |
|------------------|------------------------------------|
| LLM              | OpenAI GPT-4 / Claude / Mistral    |
| Embedding Model  | OpenAI / Sentence Transformers     |
| Vector DB        | FAISS or Chroma                    |
| Framework        | LangChain or LlamaIndex            |
| Frontend         | Streamlit                          |
| Backend          | FastAPI (optional)                 |
| Monitoring       | Python logging / Prometheus (opt)  |
| Evaluation       | Trulens, LangChain Eval, Custom    |
| Deployment       | Docker, HuggingFace Spaces         |

---

## âš™ï¸ Setup Instructions

### 1. Clone & Install

```bash
git clone https://github.com/yourname/smartdoc-qa.git
cd smartdoc-qa
pip install -r requirements.txt
```

### 2. Environment Variables

Create a `.env` file:

```
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_claude_key
```

### 3. Run App (Streamlit)

```bash
streamlit run ui/streamlit_app.py
```

---

## ğŸ§ª Evaluation

- Run offline eval:

```bash
python backend/evaluation.py
```

- Online feedback:
  - Users can rate answers in the UI  
  - Stored in `logs/feedback.json`  

---

## ğŸ“ˆ Monitoring

- Logs saved to `logs/monitor.log`  
- Tracks:  
  - Query time  
  - Token usage  
  - Tool invocation  
  - Retrieval quality  

---

## ğŸ³ Deployment

### Option 1: Docker

```bash
docker build -t smartdoc-qa .
docker run -p 8501:8501 smartdoc-qa
```

### Option 2: HuggingFace Spaces

- Add `README.md`, `app.py`, `requirements.txt`  
- Push to a public repo connected to HF Spaces  

---

## ğŸ§  Future Enhancements

- Add user auth for persistent histories  
- Plug in Langfuse or Trulens for deeper eval  
- Use OpenLLM or Ollama for local inference  

---

## ğŸ“œ License

MIT License.

---

## ğŸ‘¨â€ğŸ’» Author

Built by [Your Name] â€“ AI & LLM Applications Engineer.
